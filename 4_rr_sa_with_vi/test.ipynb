{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "124749ea72939404",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T08:57:40.417020700Z",
     "start_time": "2023-12-14T08:57:40.409021100Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from get_dataset import get_dataset\n",
    "from optimizers import Ig, Nesterov, Sgd, Shuffling\n",
    "from loss import LogisticRegression\n",
    "from utils import variance_at_opt\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def track_optimizer_loss(optimizer, x0, max_iterations):\n",
    "    x = x0.copy()\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        optimizer_trace = optimizer.run(x)\n",
    "\n",
    "        # Handle different types of outputs from the optimizer\n",
    "        if isinstance(optimizer_trace, list):\n",
    "            # For a list of outputs, compute the loss for each and take the average\n",
    "            loss_values = [optimizer.loss.value(x_i.toarray() if scipy.sparse.issparse(x_i) else x_i) for x_i in optimizer_trace]\n",
    "            avg_loss_value = np.mean(loss_values)\n",
    "            losses.append(avg_loss_value)\n",
    "        elif scipy.sparse.issparse(optimizer_trace):\n",
    "            # For sparse matrix output, convert to dense array\n",
    "            loss_value = optimizer.loss.value(optimizer_trace.toarray())\n",
    "            losses.append(loss_value)\n",
    "        else:\n",
    "            # For a single dense array output\n",
    "            loss_value = optimizer.loss.value(optimizer_trace)\n",
    "            losses.append(loss_value)\n",
    "\n",
    "    return optimizer_trace, losses\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T08:57:40.443018Z",
     "start_time": "2023-12-14T08:57:40.420020100Z"
    }
   },
   "id": "cfb990f5c20585d5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Optimizer: 100%|██████████| 600/600 [00:01<00:00, 330.70it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Nesterov\u001B[39;00m\n\u001B[0;32m     25\u001B[0m nest_str \u001B[38;5;241m=\u001B[39m Nesterov(loss\u001B[38;5;241m=\u001B[39mloss, it_max\u001B[38;5;241m=\u001B[39mn_epoch, mu\u001B[38;5;241m=\u001B[39ml2, strongly_convex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 26\u001B[0m nest_final_x, nest_losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrack_optimizer_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnest_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNesterov\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m nest_losses\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNesterov done\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[5], line 16\u001B[0m, in \u001B[0;36mtrack_optimizer_loss\u001B[1;34m(optimizer, x0, max_iterations)\u001B[0m\n\u001B[0;32m     13\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(avg_loss_value)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m scipy\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39missparse(optimizer_trace):\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m# For sparse matrix output, convert to dense array\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m     loss_value \u001B[38;5;241m=\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer_trace\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss_value)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m# For a single dense array output\u001B[39;00m\n",
      "File \u001B[1;32m~\\DataspellProjects\\OptimizationProject\\4_rr_sa_with_vi\\loss.py:47\u001B[0m, in \u001B[0;36mLogisticRegression.value\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalue\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     44\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;124;03m    Compute the value of the logistic regression loss function.\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmat_vec_product\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m     regularization \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml1 \u001B[38;5;241m*\u001B[39m safe_sparse_norm(x, \u001B[38;5;28mord\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ml2 \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m safe_sparse_norm(x) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmean(safe_sparse_multiply(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb, z) \u001B[38;5;241m-\u001B[39m logsig(z)) \u001B[38;5;241m+\u001B[39m regularization\n",
      "File \u001B[1;32m~\\DataspellProjects\\OptimizationProject\\4_rr_sa_with_vi\\loss.py:64\u001B[0m, in \u001B[0;36mLogisticRegression.mat_vec_product\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmat_vec_product\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     61\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;124;03m    Efficient matrix-vector product computation.\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_last \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39mallclose(\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtoarray\u001B[49m(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_last\u001B[38;5;241m.\u001B[39mtoarray()):\n\u001B[0;32m     65\u001B[0m         z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mA \u001B[38;5;241m@\u001B[39m x\n\u001B[0;32m     66\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmat_vec_prod \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39mtoarray()\u001B[38;5;241m.\u001B[39mravel() \u001B[38;5;28;01mif\u001B[39;00m scipy\u001B[38;5;241m.\u001B[39msparse\u001B[38;5;241m.\u001B[39missparse(z) \u001B[38;5;28;01melse\u001B[39;00m z\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "# Setup for plotting\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2, context=\"talk\", palette=sns.color_palette(\"bright\"))\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "# Get data and set parameters\n",
    "dataset = 'w8a.txt'\n",
    "A, b = get_dataset(dataset)\n",
    "loss = LogisticRegression(A, b, l1=0, l2=0)\n",
    "n, dim = A.shape\n",
    "L = loss.smoothness()\n",
    "l2 = L / np.sqrt(n)\n",
    "loss.l2 = l2\n",
    "x0 = csc_matrix((dim, 1))\n",
    "n_epoch = 600\n",
    "batch_size = 512\n",
    "n_seeds = 1  # Set to 20 in the paper\n",
    "stoch_it = 250 * n // batch_size\n",
    "\n",
    "# Run the methods and collect results\n",
    "results = {}\n",
    "\n",
    "# Nesterov\n",
    "nest_str = Nesterov(loss=loss, it_max=n_epoch, mu=l2, strongly_convex=True)\n",
    "nest_final_x, nest_losses = track_optimizer_loss(nest_str, x0, 1)\n",
    "results['Nesterov'] = nest_losses\n",
    "print('Nesterov done')\n",
    "\n",
    "# Shuffling with different configurations\n",
    "rr = Shuffling(loss=loss, lr0=1/l2, lr_max=1/loss.batch_smoothness(batch_size), lr_decay_coef=l2/3,\n",
    "               it_max=stoch_it, n_seeds=n_seeds, batch_size=batch_size)\n",
    "rr_final_x, rr_losses = track_optimizer_loss(rr, x0, stoch_it)\n",
    "results['RR'] = rr_losses\n",
    "print('RR done')\n",
    "\n",
    "so = Shuffling(loss=loss, lr0=1/l2, lr_max=1/loss.batch_smoothness(batch_size), lr_decay_coef=l2/3,\n",
    "               it_max=stoch_it, n_seeds=n_seeds, batch_size=batch_size, steps_per_permutation=np.inf)\n",
    "so_final_x, so_losses = track_optimizer_loss(so, x0, stoch_it)\n",
    "results['Shuffle-once'] = so_losses\n",
    "print('SO done')\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "sgd = Sgd(loss=loss, lr_max=1/loss.batch_smoothness(batch_size), lr0=1/l2, lr_decay_coef=l2/2,\n",
    "          it_max=stoch_it, n_seeds=1, batch_size=batch_size, avoid_cache_miss=True)\n",
    "sgd_final_x, sgd_losses = track_optimizer_loss(sgd, x0, stoch_it)\n",
    "results['SGD'] = sgd_losses\n",
    "print('SGD done')\n",
    "\n",
    "# Incremental Gradient\n",
    "ig = Ig(loss=loss, lr0=1/l2, lr_max=1/loss.batch_smoothness(batch_size), lr_decay_coef=l2/3,\n",
    "        it_max=stoch_it, batch_size=batch_size)\n",
    "ig_final_x, ig_losses = track_optimizer_loss(ig, x0, stoch_it)\n",
    "results['IG'] = ig_losses\n",
    "print('IG done')\n",
    "\n",
    "# Visualization\n",
    "plt.figure()\n",
    "for method, losses in results.items():\n",
    "    plt.plot(losses, label=method)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Optimization Loss over Iterations')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-14T08:57:42.488308100Z",
     "start_time": "2023-12-14T08:57:40.434018500Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results['RR'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T08:57:42.485308300Z"
    }
   },
   "id": "1ac1d5415c048c63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Continuing from the previous part...\n",
    "\n",
    "# Measure variance\n",
    "batches = [1, 4, 8] + list(np.unique(np.logspace(1, np.log10(loss.n // 30), num=50, dtype=int)))\n",
    "n_perms = 10\n",
    "vars_sgd = []\n",
    "vars_rr = []\n",
    "vars_rr_upper = []\n",
    "vars_rr_lower = []\n",
    "for b in batches:\n",
    "    var_sgd, var_rr, var_rr_upper, var_rr_lower = variance_at_opt(x_opt, loss, batch_size=b, n_perms=n_perms)\n",
    "    vars_sgd.append(var_sgd)\n",
    "    vars_rr.append(var_rr)\n",
    "    vars_rr_upper.append(var_rr_upper)\n",
    "    vars_rr_lower.append(var_rr_lower)\n",
    "\n",
    "# Visualize variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batches, vars_sgd, label=r'$\\sigma_{*}^2$')\n",
    "plt.plot(batches, vars_rr_upper, label=r'$\\frac{\\gamma L n}{4}\\sigma_{*}^2$, $\\gamma=\\frac{1}{L}$')\n",
    "plt.plot(batches, vars_rr_lower, label=r'$\\frac{\\gamma \\mu n}{8}\\sigma_{*}^2$, $\\gamma=\\frac{1}{L}$')\n",
    "plt.plot(batches, vars_rr, label=r'$\\sigma_{\\mathrm{shuffe}}^2$, $\\gamma=\\frac{1}{L}$')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Batch size')\n",
    "plt.ylabel('Variance at $x_{\\star}$')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T08:57:42.487309200Z"
    }
   },
   "id": "41bf7048eb5c861c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histogram visualization\n",
    "n_perms = 1\n",
    "vars_so = []\n",
    "for _ in range(1000):\n",
    "    lr = 1 / loss.max_smoothness()\n",
    "    _, var_so, _, _ = variance_at_opt(x_opt, loss, batch_size=batch, n_perms=n_perms, lr=lr)\n",
    "    vars_so.append(var_so)\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "bins = np.logspace(np.log10(np.min(vars_so)), np.log10(np.max(vars_so)), 45)\n",
    "plt.hist(vars_so, bins=bins, edgecolor='#e0e0e0', linewidth=0., alpha=0.7, log=True)\n",
    "plt.xlabel(r'Variance at $x_{\\star}$')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xscale('log')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-14T08:57:42.488308100Z"
    }
   },
   "id": "e1435ce0a3d4fa13"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
