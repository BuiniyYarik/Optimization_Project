{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on \"Improved Analysis and Rates for Variance Reduction under Without-replacement Sampling Orders\"\n",
    "\n",
    "__Authors__: Xinmeng Huang, Kun Yuan, Xianghui Mao, Wotao Yin\n",
    "\n",
    "__Link__: https://arxiv.org/abs/2104.12112\n",
    "\n",
    "## 1. General idea\n",
    "In the given paper variance reduction under without-replacement sampling methods are analysed. This paper propose method called `Prox-DFinito` which is variant of [Finito](https://arxiv.org/pdf/1407.2710.pdf), which improves converges rates for without-replacement approaches.\n",
    "\n",
    "## 2. Problem statement\n",
    "Strictly speaking our problem is Finite-sum composite optimization problem\n",
    "$$\n",
    "\\min_{x\\in R^d} F(x) + r(x)\n",
    "$$\n",
    "$$\n",
    "F(x) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $f_i(x)$ is convex and differentiable\n",
    "- $r(x)$ is convex only\n",
    "\n",
    "Intuition in Machine Learning:\n",
    "- $x$ is weights of model\n",
    "- $f_i(x)$ is loss function associated with $i$-th data point\n",
    "- $r(x)$ is regularization function\n",
    "- $F(x)$ is average loss on data\n",
    "\n",
    "## 3. Useful Definitions/Notation\n",
    "\n",
    "\n",
    "### Sampling methods\n",
    "Without replacement sampling garantues that each $f_i(x)$ will be presented only once during one epoch.\n",
    "Without replacement methods which are considered in this paper:\n",
    "- `Cyclic sampling`: $\\pi = (\\pi(1), \\pi(2), \\dots ,\\pi(n))$ is random determined permutation of sample indexes. This order $\\pi$ is same for all epochs\n",
    "- `Random reshuffling`: every epoch new random permutation $\\tau = (\\tau(1), \\tau(2), \\dots, \\tau(n))$ \n",
    "\n",
    "### Notation\n",
    "- $a^{b}$, $b$ means not power but iteration number.\n",
    "\n",
    "- __proximal operator__:\n",
    "$$\n",
    "prox_{\\alpha r} (x) = argmin_{y \\in R^d}(\\alpha r(y) + \\frac{1}{2} || y - x ||^2)\n",
    "$$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "- $n$ - number of data rows\n",
    "- $d$ - number of weights in single model to optimize\n",
    "- Lets define $z_i$ as weights vector for single data row $i$. Thus, $z \\in R^{n, d}$. Initially, can be random\n",
    "- Define $\\bar z$ as average weight: \n",
    "$$\\bar z = \\frac{1}{n} \\sum_{i=1}^n z_i $$\n",
    "- $x \\in R^{d}$ as our final weights for model\n",
    "- $\\alpha$ is learning rate\n",
    "- $\\theta$ is hyperparameter (see below for more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "1: & \\ \\textbf{for epoch } k = 0, 1, 2, \\dots \\textbf{do} \\\\\n",
    "2: & \\quad \\textbf{for iteration } t = kn + 1, kn + 2, \\dots, (k+1)n \\textbf{ do} \\\\\n",
    "3: & \\quad \\quad x^{t-1} = \\textbf{prox}_{\\alpha r} (\\bar z^{t-1}) \\\\\n",
    "4: & \\quad \\quad \\text{Pick } i_{t} \\text{with some rule} \\\\\n",
    "5: & \\quad \\quad \\text{Update } z_{i_t}^t \\text{ and } \\bar z^t \\text{ according to (1) and (2)} \\\\\n",
    "6: & \\quad \\textbf{end for} \\\\\n",
    "7: & \\quad z_i^{(k+1)n} \\xleftarrow{} (1 - \\theta) z_i^{kn} + \\theta z_i^{(k+1)n} \\text{ for any } i \\in [n] \\\\\n",
    "8: & \\quad \\bar{z^{(k+1)n}} \\xleftarrow{} (1 - \\theta) z_i^{kn} + \\theta \\bar z^{(k+1)n} \\\\\n",
    "9: &  \\ \\textbf{end for}\n",
    "\\end{align*}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration\n",
    "\n",
    "Idea of iteration is $x$ is computed as compromise ($prox_{\\alpha r}$) between regularisation and some kind of average of best weights ($\\bar z$) for every data row.\n",
    "\n",
    "length of interval $[kn+1; (k+1)n]$ is $n$. It is written in this manner to store previous epoch in same iteration manner.\n",
    "\n",
    "On every iteration $t$ we compute $x^{t-1}$ as proximal of $\\bar z^{t-1}$ and regularisation function. \n",
    "\n",
    "Then we choose next data row index $i_t$, which can be not in numerical order. For example, if random shuffling is used.\n",
    "\n",
    "\n",
    "Then we update:\n",
    "$$\n",
    "\\begin{align}\n",
    "    z_i^t &=\n",
    "    \\begin{cases}\n",
    "        x^{t-1} - \\alpha \\nabla f_i(x^{t-1}), & \\text{ if } i = i_t, \\\\\n",
    "        z_i^{t-1}, & \\text{ if } i \\neq i_t\n",
    "    \\end{cases} \\\\\n",
    "    \\bar z^t &= \\bar z^{t-1} + \\frac{z_{i_t}^t - z_{i_t}^{t-1}}{n}\n",
    "\\end{align}\n",
    "$$\n",
    "(1) can be interpreted as updating only $z_{i_t}^t$\n",
    "\n",
    "### Epoch end\n",
    "\n",
    "After all iterations, we perform a 'damping' step.\n",
    "\n",
    "$$\n",
    "z_i^{(k+1)n} = (1 - \\theta) z^{kn}_i + \\theta z^{(k+1)n}_i\\\\\n",
    "\\bar z^{(k+1)n} = (1 - \\theta) \\bar z^{kn} + \\theta \\bar z^{(k+1)n}\\\\\n",
    "$$\n",
    "\n",
    "Here we update last weights matrix as weighted sum of first and last iteration weights.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
