{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Report on \"Random Reshuffling: Simple Analysis with Vast Improvements\"\n",
    "__Authors__: Konstantin Mishchenko, Ahmed Khaled, Peter Richtarik\n",
    "__Link__: [https://proceedings.neurips.cc/paper/2020/file/c8cc6e90ccbff44c9cee23611711cdc4-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/c8cc6e90ccbff44c9cee23611711cdc4-Paper.pdf)\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 1. Problem Statement\n",
    "\n",
    "The paper addresses the finite-sum minimization problem, which is a common task in machine learning and statistics. Specifically, it deals with stochastic gradient method for minimizing a finite-sum objective function:\n",
    "\n",
    "$$\n",
    "F(x) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x \\in R^d$ is a vector representing the parameters (model weights, features) of a model we wish to train;\n",
    "- $n$ is the total number of training data points;\n",
    "- $f_i(x)$ is the (smooth) loss associated with the model on the $i$-th data point.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 2. Importance\n",
    "\n",
    "The described finite-sum minimization problem is critical in various fields, particularly in machine learning, as first-order methods, known for their scalability and low memory requirements, are widely used. The paper concentrates on the role of data reshuffling in these methods.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 3. Examples of Occurrence\n",
    "\n",
    "The problem is prevalent in machine learning, especially in tasks involving large datasets and complex models. Practical examples include:\n",
    "- __Training Deep Neural Networks__;\n",
    "- __Matrix Factorization__;\n",
    "- __Regression Problems__, where minimizing the sum of individual loss functions is a common objective.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 4. Authors' Approach\n",
    "\n",
    "The primary approach in this paper is to investigate the convergence behavior of algorithms based on random or deterministic data reshuffling, such as Random Reshuffling (RR), Shuffle-Once (SO), and Incremental Gradient (IG). The primary basis for this approach lies in the fundamental idea of reshuffling data for optimization purposes. The authors introduce a new concept called \"shuffling variance\" to understand the behavior of these algorithms:\n",
    "\n",
    "_Given a stepsize_ $\\gamma > 0$ _and a random permutation_ $\\pi$ _of_ $\\{1, 2, \\ldots, n\\}$, _the shuffling variance is given by_:\n",
    "\\begin{align}\n",
    "\\sigma^2_{\\text{Shuffle}} &= \\max_{i=1,\\ldots,n-1} \\frac{1}{\\gamma} \\mathbb{E}\\left[Df_{\\pi_i}(x_i^*, x^*)\\right]\n",
    "\\end{align}\n",
    "_where_:\n",
    "- _the expectation is taken with respect to the randomness in the permutation_ $\\pi$;\n",
    "- $Df_i(x, y) = f_i(x) - f_i(y) - \\langle \\nabla f_i(y), x - y \\rangle$ _is the Bregman divergence between_ $x$ _and_ $y$ _associated with_ $f_i$;\n",
    "- $x_i^* = x^* - \\gamma \\sum_{j=0}^{i-1} \\nabla f_{\\pi_j}(x^*)$ _is the real limit points of the algorithm_.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 5. The Proposed Methods\n",
    "\n",
    "- __Random Reshuffling (RR)__: In each epoch, RR generates a new random permutation of the data and performs gradient descent steps using this permutation. This reshuffling process differentiates RR from SGD.\n",
    "- __Shuffle-Once (SO)__: Similar to RR, SO shuffles the dataset, but it does so only once at the beginning and reuses the same permutation in all subsequent epochs.\n",
    "- __Incremental Gradient (IG)__: IG is identical to SO, but the initial permutation is deterministic, which means it performs incremental gradient steps through the data in a cycling fashion.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## 6. Algorithms\n",
    "\n",
    "#### a) Random Reshuffling (RR)\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\textbf{Input:} & \\text{ Stepsize } \\gamma > 0, \\text{ initial vector } x_0 = x_0^0 \\in \\mathbb{R}^d, \\text{ number of epochs } T. \\\\\n",
    "1: & \\text{for epochs } t = 0, 1, \\ldots, T - 1 \\text{ do} \\\\\n",
    "2: & \\quad \\text{Sample a permutation } \\{\\pi_0, \\pi_1, \\ldots, \\pi_{n-1}\\} \\text{ of } \\{1, 2, \\ldots, n\\} \\\\\n",
    "3: & \\quad \\text{for } i = 0, 1, \\ldots, n - 1 \\text{ do} \\\\\n",
    "4: & \\quad \\quad x_{i+1}^t = x_i^t - \\gamma \\nabla f_{\\pi_i}(x_i^t) \\\\\n",
    "5: & \\quad x_{t+1} = x_n^t \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "\n",
    "#### b) Shuffle-Once (SO)\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\textbf{Input:} & \\text{ Stepsize } \\gamma > 0, \\text{ initial vector } x_0 = x_0^0 \\in \\mathbb{R}^d, \\text{ number of epochs } T. \\\\\n",
    "1: & \\text{Sample a permutation } \\{\\pi_0, \\pi_1, \\ldots, \\pi_{n-1}\\} \\text{ of } \\{1, 2, \\ldots, n\\} \\\\\n",
    "2: & \\text{for epochs } t = 0, 1, \\ldots, T - 1 \\text{ do} \\\\\n",
    "3: & \\quad \\text{for } i = 0, 1, \\ldots, n - 1 \\text{ do} \\\\\n",
    "4: & \\quad \\quad x_{i+1}^t = x_i^t - \\gamma \\nabla f_{\\pi_i}(x_i^t) \\\\\n",
    "5: & \\quad x_{t+1} = x_n^t \\\\\n",
    "\\end{align*}\n",
    "$\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## 7. Intuition of Effectiveness\n",
    "\n",
    "The effectiveness of these methods is rooted in their ability to leverage reshuffling for better convergence. By introducing the notion of shuffling variance, the paper explains why RR can outperform SGD. This variance accounts for the algorithms' behaviors and contributes to their superior performance in various practical scenarios."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "564a66b34494e6f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
