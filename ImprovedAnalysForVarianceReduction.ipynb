{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General idea\n",
    "In the given paper variance reduction under without-replacement sampling methods are analysed. Popular without replacement sampling approaches are cyclic sampling, random reshuffling and shuffling once. This paper propose method called `Prox-DFinito` which is variant of [Finito](https://arxiv.org/pdf/1407.2710.pdf), which improves converges rates for without-replacement approaches.\n",
    "\n",
    "## Problem statement\n",
    "Strictly speaking our problem is Finite-sum composite optimization problem\n",
    "$$\n",
    "\\min_{x\\in R^d} F(x) + r(x)\n",
    "$$\n",
    "$$\n",
    "F(x) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n",
    "$$\n",
    "\n",
    "Where each $f_i(x)$ is convex and differentiable, $r(x)$ is regularisation function, which is convex but not necessary differentiable.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "We can interpret $f_i(x)$ in Machine Learning as follows. $f$ is our loss function, weights of model is $x$ and $i$ is index of data row that we pass to our model. Our goal is to minimize average loss on our data with some regularisation in mind.\n",
    "\n",
    "#### Useful Definitions/Notation\n",
    "\n",
    "\n",
    "#### Sampling methods\n",
    "Without replacement sampling garantues that each $f_i(x)$ will be presented only once during one epoch.\n",
    "Without replacement methods which are considered in this paper:\n",
    "- `Cyclic sampling`: $\\pi = (\\pi(1), \\pi(2), \\dots ,\\pi(n))$ is random determined permutation of sample indexes. This order $\\pi$ is same for all epochs\n",
    "- `Random reshuffling`: every epoch new random permutation $\\tau = (\\tau(1), \\tau(2), \\dots, \\tau(n))$ \n",
    "\n",
    "#### Notation\n",
    "When we use $a^{b}$, $b$ means not power but iteration number\n",
    "\n",
    "Let's define proximal operator\n",
    "$$\n",
    "prox_{\\alpha r} (x) = argmin_{y \\in R^d}(\\alpha r(y) + \\frac{1}{2} || y - x ||^2)\n",
    "$$\n",
    "Idea of operator is to choose such $y$ thay satisfy closeness to $x$ and regularisation term with respect to some hyperparameter $\\alpha$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "$n$ - number of data rows\n",
    "\n",
    "$d$ - number of weights in single model to optimize\n",
    "\n",
    "Lets define $z_i$ as weights vector for single data row $i$. Thus, $z \\in R^{n, d}$. Initially, can be random\n",
    "\n",
    "Define $\\bar z$ as: \n",
    "$$\\bar z = \\frac{1}{n} \\sum_{i=1}^n z_i $$\n",
    "\n",
    "Define $x \\in R^{d}$ as our final weights for model\n",
    "\n",
    "$\\alpha$ is learning rate\n",
    "\n",
    "$\\theta$ is hyperparameter (see below for more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./IAVFR_alg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's breakdown algorithm by parts:\n",
    "\n",
    "### Iteration\n",
    "\n",
    "Idea of iteration is $x$ is computed as compromise ($prox_{\\alpha r}$) between regularisation and some kind of average of best weights ($\\bar z$) for every data row.\n",
    "\n",
    "length of interval $[kn+1; (k+1)n]$ is $n$. It is written in this manner to store previous epoch in same iteration manner.\n",
    "\n",
    "On every iteration $t$ we compute $x^{t-1}$ as proximal of $\\bar z^{t-1}$ and regularisation function. \n",
    "\n",
    "Then we choose next data row index $i_t$, which can be not in numerical order. For example, if random shuffling is used.\n",
    "\n",
    "\n",
    "Then we update:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    z_i^t &=\n",
    "    \\begin{cases}\n",
    "        x^{t-1} - \\alpha \\nabla f_i(x^{t-1}), & \\text{ if } i = i_t, \\\\\n",
    "        z_i^{t-1}, & \\text{ if } i \\neq i_t\n",
    "    \\end{cases} \\\\\n",
    "    \\bar z^t &= \\bar z^{t-1} + \\frac{z_{i_t}^t - z_{i_t}^{t-1}}{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "(1) can be interpreted as updating only $z_{i_t}^t$\n",
    "\n",
    "### Epoch end\n",
    "\n",
    "After all iterations, we perform a 'damping' step.\n",
    "\n",
    "$$\n",
    "z_i^{(k+1)n} = (1 - \\theta) z^{kn}_i + \\theta z^{(k+1)n}_i\\\\\n",
    "\\bar z^{(k+1)n} = (1 - \\theta) \\bar z^{kn} + \\theta \\bar z^{(k+1)n}\\\\\n",
    "$$\n",
    "\n",
    "Here we update last weights matrix as weighted sum of first and last iteration weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
